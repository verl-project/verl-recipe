# Format checks enforced on CI:
# 1. Comments must appear above each field.
# 2. There must be a blank line between each field.
# 3. Inline comments (after a field on the same line) are not allowed.
# 4. Indentation level is respected for nested fields.

# specify the default per-component configs
defaults:

  # <folder_name>@<field_name>.<field_name>: <yaml_file_name>
  # actor_rollout_ref.actor: trainer/config/actor/dp_actor.yaml
  - actor@actor_rollout_ref.actor: dp_actor

  # data: trainer/config/data/legacy_data.yaml
  - data@data: legacy_data

  # Reference model config.
  # Reference model will be enabled when actor.use_kl_loss or/and algorithm.use_kl_in_reward is/are True.
  - ref@actor_rollout_ref.ref: dp_ref

  # Rollout model config.
  - rollout@actor_rollout_ref.rollout: rollout

  # Critic model config.
  - critic@critic: dp_critic

  # Reward model config.
  - reward_model@reward_model: dp_reward_model

  # load the reference default config, then apply the fields in the current yaml
  # self config override anything above
  - _self_


data:

  tokenizer: null

  train_files: ~/data/rlhf/gsm8k/train.parquet

  val_files: null

  prompt_key: question

  reward_fn_key: data_source

  thinking_ratio: 0.5

  max_prompt_length: 512

  max_response_length: 512

  train_batch_size: 1024

  val_batch_size: null

  return_raw_input_ids: False

  return_raw_chat: False

  return_full_prompt: False

  shuffle: True

  filter_overlong_prompts: False

  filter_overlong_prompts_workers: 192

  truncation: error

  trust_remote_code: False

  apply_chat_template_kwargs:

    enable_thinking: false

  custom_cls:

      path: null

      name: null

  sampler:

    class_path: null

    class_name: null

  dataloader_num_workers: 8

  return_multi_modal_inputs: True


actor_rollout_ref:

  hybrid_engine: True

  nccl_timeout: 600

  model:

    path: ~/models/deepseek-llm-7b-chat

    custom_chat_template: null

    use_shm: false

    external_lib: null

    override_config: {}

    enable_gradient_checkpointing: false

    enable_activation_offload: false

    use_remove_padding: False

    lora_rank: 0

    lora_alpha: 16

    target_modules: all-linear

    exclude_modules: null

    use_liger: false

    use_fused_kernels: false

    fused_kernel_options:

      impl_backend: torch

    trust_remote_code: False

  actor:

    # Whether to automatically adjust batch size at runtime
    strategy: fsdp

    ppo_epochs: 1

    ppo_mini_batch_size: 16

    ppo_micro_batch_size: 16

    ppo_micro_batch_size_per_gpu: 16

    topk: null

    kd_loss_type: null

    jsd_alpha: null

    power_alpha: null

    fsdp_config:

      fsdp_size: -1

      param_offload: False

      optimizer_offload: False

    optim:

      # Learning rate
      lr: 1e-6

      warmup_style: constant

    data_loader_seed: null

    load_weight: True

    checkpoint: 

      async_save: False

      # What to include in saved checkpoints
      # with 'hf_model' you can save whole model as hf format, now only use sharded model checkpoint to save space
      save_contents: ['model', 'optimizer', 'extra']

      load_contents: ${actor_rollout_ref.actor.checkpoint.save_contents}

    use_dynamic_bsz: False

    max_token_len: 1024

    use_torch_compile: False

    shuffle: False

    # profile the actor model in `update_policy` 

  rollout:

    _target_: verl.workers.config.RolloutConfig

    name: vllm

    mode: sync

    enable_chunked_prefill: True

    load_format: safetensors

    layered_summon: False

    temperature: 1.0

    top_k: -1

    top_p: 1

    n: 1

    data_parallel_size: 1

    prompt_length: ${data.max_prompt_length}

    response_length: ${data.max_response_length}

    dtype: bfloat16

    gpu_memory_utilization: 0.5

    log_prob_micro_batch_size: 16

    log_prob_micro_batch_size_per_gpu: 16

    tensor_model_parallel_size: 1


  ref:

    teacher_path: null

    log_prob_micro_batch_size: null

    log_prob_micro_batch_size_per_gpu: 16

    fsdp_config:

      fsdp_size: -1

      param_offload: False

      optimizer_offload: False
  

trainer:

  balance_batch: True

  total_epochs: 30

  total_training_steps: null

  profile_steps: null

  project_name: verl_examples

  experiment_name: gsm8k

  logger: ['console']

  log_val_generations: 0

  rollout_data_dir: null

  validation_data_dir: null

  nnodes: 1

  n_gpus_per_node: 1

  ref_n_gpus_per_node: null

  ref_nnodes: null

  save_freq: -1

  test_freq: null

  esi_redundant_time: 0

  resume_mode: auto

  resume_from_path: null

  val_before_train: True

  val_only: False

  critic_warmup: 0

  default_hdfs_dir: null

  del_local_ckpt_after_load: False

  default_local_dir: null

  max_actor_ckpt_to_keep: null

  max_critic_ckpt_to_keep: null

  ray_wait_register_center_timeout: 300

  device: cuda

  use_legacy_worker_impl: auto

  wandb_proxy: null


global_profiler:

  _target_: verl.utils.profiler.ProfilerConfig

  tool: null

  steps: null

  profile_continuous_steps: False

  save_path: "outputs/profile"

  global_tool_config:
  
    nsys:

      _target_: verl.utils.profiler.config.NsightToolConfig

      discrete: False

      controller_nsight_options:

        trace: "cuda,nvtx,cublas,ucx"

        cuda-memory-usage: "true"

        cuda-graph-trace: "graph"

      worker_nsight_options:

        trace: "cuda,nvtx,cublas,ucx"

        cuda-memory-usage: "true"

        cuda-graph-trace: "graph"

        capture-range: "cudaProfilerApi"

        capture-range-end: null

        kill: none


    torch_memory:

      trace_alloc_max_entries: 100_000

      stack_depth: 32

      context: "all"

      stacks: "all"

      kw_args: {}


critic:

  enable: False

reward_model:

  use_reward_loop: False

algorithm:

  use_kl_in_reward: False


ray_init:

  num_cpus: null

  timeline_json_file: null
